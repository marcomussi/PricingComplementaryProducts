{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fab7bb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ab13094-f005-43d0-a1d4-119c0c203c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndependentPricingEnv: \n",
    "    \n",
    "    def __init__(self, actions, curves, n_users, seed=0):\n",
    "        assert curves.shape == actions.shape, \"Shape of the action not coherent\"\n",
    "        self.n_products = actions.shape[0]\n",
    "        self.n_actions = actions.shape[1]\n",
    "        self.n_users = n_users\n",
    "        self.actions = actions # dim 0: product; dim 1: actions\n",
    "        self.curves = curves # dim 0: product; dim 1: actions\n",
    "        self.actions_to_idx = {}\n",
    "        for itm in range(self.n_products):\n",
    "            self.actions_to_idx[itm] = {}\n",
    "        for itm in range(self.n_products):\n",
    "            for i, act in enumerate(self.actions[itm, :]):\n",
    "                self.actions_to_idx[itm][act] = i\n",
    "        self.reset(seed)  \n",
    "    \n",
    "    def step(self, action, convrate_only=True):\n",
    "        assert action.ndim == 1, \"The action must be 1-dimensional\"\n",
    "        assert action.shape[0] == self.n_products, \"The action must be of dimension n_products\"\n",
    "        sales = np.zeros((self.n_products, self.n_users))\n",
    "        for itm in range(self.n_products):\n",
    "            sales[itm, :] = self.curves[itm, self.actions_to_idx[itm][action[itm]]]\n",
    "        sales = sales > np.random.uniform(0, 1, sales.shape)\n",
    "        if convrate_only:\n",
    "            return sales.sum(axis=1) / self.n_users\n",
    "        else: \n",
    "            sales\n",
    "                \n",
    "    \n",
    "    def reset(self, seed=0):\n",
    "        np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "904f957c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimisticKernelizedBanditAgent: \n",
    "\n",
    "    \n",
    "    def __init__(self, horizon, actions, sigma=0.5, make_plots=True):\n",
    "        self.horizon = horizon\n",
    "        self.sigma_process = sigma  \n",
    "        self.actions = actions # shape (action_number, action_dimension)\n",
    "        self.action_dim = self.actions.shape[1]\n",
    "        self.n_actions = self.actions.shape[0]\n",
    "        self.make_plots = make_plots\n",
    "        self.action_enum = np.linspace(0, self.n_actions-1, self.n_actions, dtype=int)\n",
    "        self.reset()\n",
    "\n",
    "        \n",
    "    def pull(self):\n",
    "        if self.nosamples:\n",
    "            self.last_action = self.actions[np.random.choice(self.action_enum), :]\n",
    "        else:\n",
    "            beta = np.log(self.horizon)\n",
    "            self.last_action = self.actions[np.argmax(self.mu + beta * self.sigma)]\n",
    "        return self.last_action\n",
    "            \n",
    "    \n",
    "    def update(self, reward):\n",
    "        if self.x_vect is None:\n",
    "            self.x_vect = np.array([self.last_action]).reshape(1, self.action_dim)\n",
    "            self.y_vect = np.array([reward]).reshape(1, 1)\n",
    "        else:\n",
    "            self.x_vect = np.vstack((self.x_vect, np.array([self.last_action]).reshape(1, self.action_dim)))\n",
    "            self.y_vect = np.vstack((self.y_vect, np.array([reward]).reshape(1, 1)))\n",
    "\n",
    "    \n",
    "    def compute(self):\n",
    "        k = rbf(self.x_vect, self.x_vect)\n",
    "        k = k + (self.sigma_process * np.eye(self.y_vect.shape[0]))\n",
    "        k_inv = np.linalg.inv(k)  # can be improved (see, e.g., https://doi.org/10.1016/S0898-1221(01)00278-4)\n",
    "        self.mu = np.zeros(self.n_actions)\n",
    "        self.sigma = np.zeros(self.n_actions)\n",
    "        for i, x_plt_i in enumerate(list(self.actions)):\n",
    "            k_star = rbf(self.x_vect, np.array([x_plt_i]).reshape(1, self.action_dim))\n",
    "            self.mu[i] = k_star.T @ k_inv @ self.y_vect\n",
    "            self.sigma[i] = 1 - k_star.T @ k_inv @ k_star\n",
    "        self.nosamples = False\n",
    "        return self.actions, self.mu, self.sigma\n",
    "\n",
    "    \n",
    "    def reset(self):\n",
    "        self.nosamples = True\n",
    "        self.x_vect = None\n",
    "        self.y_vect = None\n",
    "    \n",
    "\n",
    "    def compute(self):\n",
    "        k = rbf(self.x_vect, self.x_vect)\n",
    "        k = k + (self.sigma_process * np.eye(self.y_vect.shape[0]))\n",
    "        k_inv = np.linalg.inv(k)  # can be improved (see, e.g., https://doi.org/10.1016/S0898-1221(01)00278-4)\n",
    "        self.mu = np.zeros(self.n_actions)\n",
    "        self.sigma = np.zeros(self.n_actions)\n",
    "        for i, x_plt_i in enumerate(list(self.actions)):\n",
    "            k_star = rbf(self.x_vect, np.array([x_plt_i]).reshape(1, self.action_dim))\n",
    "            self.mu[i] = k_star.T @ k_inv @ self.y_vect\n",
    "            self.sigma[i] = 1 - k_star.T @ k_inv @ k_star\n",
    "        self.nosamples = False\n",
    "        return self.actions, self.mu, self.sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "402cb832",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Runner: \n",
    "\n",
    "    \n",
    "    def __init__(self, agent, env):\n",
    "        self.agent = agent\n",
    "        self.env = env\n",
    "\n",
    "    \n",
    "    def run_simulations(self, horizon, n_runs, draw=False, draw_every=10): \n",
    "        # actions are in [0, 1] for simplicity\n",
    "        for run_i in range(n_runs):\n",
    "            self.agent.reset()\n",
    "            self.env.reset(seed=run_i)\n",
    "            plt.figure()\n",
    "            cols = 2\n",
    "            f, ax = plt.subplots(int(horizon/(draw_every)), cols, figsize=(cols*6,horizon/draw_every*4))\n",
    "            actions = np.zeros((horizon, self.agent.action_dim))\n",
    "            rewards = np.zeros(horizon)\n",
    "            plot_count = 0\n",
    "            for t in tqdm(range(horizon)):\n",
    "                actions[t, :] = self.agent.pull()\n",
    "                rewards[t] = self.env.step(actions[t, :])\n",
    "                self.agent.update(rewards[t])\n",
    "                if draw and t % draw_every == 0:\n",
    "                    # int(t/cols), t % cols\n",
    "                    x_plt, mu, sigma = self.agent.compute()\n",
    "                    ax[plot_count, 0].plot(x_plt.ravel(), mu)\n",
    "                    ax[plot_count, 0].plot(x_plt.ravel(), self.env.mean0(x_plt))\n",
    "                    ax[plot_count, 0].fill_between(x_plt.ravel(), mu-sigma, mu+sigma, alpha=0.3)\n",
    "                    ax[plot_count, 0].set_title(\"t = \" + str(t+1))\n",
    "                    ax[plot_count, 0].scatter(actions[:t+1], rewards[:t+1])\n",
    "                    ax[plot_count, 0].set_xlim([0, 1])\n",
    "                    ax[plot_count, 0].set_ylim([-0.1, 1.1])\n",
    "                    ax[plot_count, 1].hist(actions[:t+1], bins=11, alpha=0.1)\n",
    "                    ax[plot_count, 1].set_xlim([0, 1])\n",
    "                    for act in np.unique(actions[:t+1]):\n",
    "                        avg = rewards[:t+1]\n",
    "                        mask = actions[:t+1] == act\n",
    "                        avg = np.mean(avg[mask.ravel()])\n",
    "                        ax[plot_count, 0].scatter(act, avg, color=\"r\")\n",
    "                    plot_count = plot_count + 1\n",
    "            \"\"\"\n",
    "            fig = plt.figure()\n",
    "            ax = fig.add_subplot(projection='3d')\n",
    "            ax.scatter(x_plt[:, 0], x_plt[:, 1], mu, marker=\"o\")\n",
    "            ax.scatter(x_plt[:, 0], x_plt[:, 1], \n",
    "                       self.env.mean0(x_plt[:, 0]) * self.env.mean1(x_plt[:, 1]), marker=\"x\")\n",
    "            \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "630173de",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BernoulliEnvironmentTwoDims' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 16\u001b[0m\n\u001b[1;32m     11\u001b[0m actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack((actions0\u001b[38;5;241m.\u001b[39mravel()\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), actions1\u001b[38;5;241m.\u001b[39mravel()\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)))\n\u001b[1;32m     14\u001b[0m agent \u001b[38;5;241m=\u001b[39m OptimisticKernelizedBanditAgent(horizon\u001b[38;5;241m=\u001b[39mhorizon,\n\u001b[1;32m     15\u001b[0m         actions\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m11\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 16\u001b[0m env2d \u001b[38;5;241m=\u001b[39m BernoulliEnvironmentTwoDims(mean0, mean1)\n\u001b[1;32m     17\u001b[0m env1d \u001b[38;5;241m=\u001b[39m BernoulliEnvironmentOneDim(mean0)\n\u001b[1;32m     18\u001b[0m runner \u001b[38;5;241m=\u001b[39m Runner(agent, env1d)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BernoulliEnvironmentTwoDims' is not defined"
     ]
    }
   ],
   "source": [
    "#Â %matplotlib qt\n",
    "\n",
    "horizon = 200\n",
    "\n",
    "mean0 = lambda x : (np.power(x, 2) - np.power(x, 3)) * 2 + 0.3\n",
    "mean1 = lambda x : (np.power(1-x, 2) - np.power(1-x, 3)) * 2 + 0.3\n",
    "\n",
    "actions0 = np.linspace(0, 1, 6)\n",
    "actions1 = np.linspace(0, 1, 6)\n",
    "actions0, actions1 = np.meshgrid(actions0, actions1)\n",
    "actions = np.hstack((actions0.ravel().reshape(-1, 1), actions1.ravel().reshape(-1, 1)))\n",
    "\n",
    "\n",
    "agent = OptimisticKernelizedBanditAgent(horizon=horizon,\n",
    "        actions=np.linspace(0, 1, 11).reshape(-1, 1))\n",
    "env2d = BernoulliEnvironmentTwoDims(mean0, mean1)\n",
    "env1d = BernoulliEnvironmentOneDim(mean0)\n",
    "runner = Runner(agent, env1d)\n",
    "runner.run_simulations(horizon=horizon, n_runs=1, draw=True, draw_every=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
